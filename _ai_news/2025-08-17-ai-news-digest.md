---
date: '2025-08-17'
stories:
- source: Reuters
  summary: 'Alphabet’s Google announced a $9 billion investment to build a new data‑center
    campus in Stillwater and expand its Pryor facility, part of a broader push to
    scale U.S. cloud and AI capacity. The package also ties into Google’s $1 billion
    AI education and training initiative for U.S. universities and nonprofits. Why
    it matters: the spend signals continued heavy capital allocation by Big Tech into
    on‑premises AI infrastructure — accelerating competition for compute, talent and
    regional economic development while reinforcing the trend of AI onshoring in the
    U.S.'
  title: Google pledges $9 billion to boost AI and cloud infrastructure in Oklahoma
  url: https://www.reuters.com/business/google-pledges-9-billion-expand-ai-cloud-infrastructure-oklahoma-2025-08-13/
- source: Financial Times
  summary: 'The Financial Times reports that Sam Altman is backing a new venture,
    Merge Labs, that aims to develop high‑bandwidth brain‑computer interfaces as a
    rival to Neuralink. The move represents a major new entrant and funding interest
    in the brain‑computer interface sector from one of AI’s most prominent investors.
    Why it matters: if undertaken at scale, the project could accelerate integration
    between advanced AI and neural hardware, drawing fresh capital and regulatory
    attention to an already high‑stakes area of tech innovation.'
  title: Sam Altman backs Merge Labs to take on Neuralink in brain‑computer interfaces
  url: https://www.ft.com/content/04484164-724e-4fc2-92a2-e2c13ea639bd
- source: arXiv
  summary: 'What happened: A Microsoft Research / UW team posted an arXiv preprint
    (Aug 13, 2025) titled “Sample More to Think Less: Group Filtered Policy Optimization
    (GFPO).” The paper introduces GFPO, a reinforcement‑learning training method for
    post‑training/online fine‑tuning of reasoning models that samples larger groups
    of candidate answers during training and filters them by length and token‑efficiency
    (reward per token). Why it matters: common RL recipes for reasoning (e.g., GRPO)
    often drive models to inflate answers — trading extra tokens for marginal accuracy
    gains. GFPO explicitly optimizes for token‑efficient reward and reports large
    reductions in length inflation (reported 46–85% reductions on several reasoning
    and coding benchmarks) while preserving accuracy. Impact: GFPO points to a practical
    trade — spend more compute at training time to cut inference cost and latency
    — that could make reasoning‑specialized models far cheaper and faster in deployment,
    and help teams building resource‑sensitive LLMs (education, edge, cost‑constrained
    apps). The paper also proposes adaptive difficulty allocation to focus training
    compute where it most reduces inference burden.'
  title: Sample More to Think Less — Microsoft researchers publish GFPO to cut RL-induced
    verbosity in LLM reasoning
  url: https://arxiv.org/abs/2508.09726
- source: arXiv
  summary: 'What happened: A large team (NextStep) released an arXiv preprint (Aug
    14, 2025) introducing NextStep‑1, a 14B autoregressive text→image model that trains
    on discrete text tokens and continuous image tokens (with a 157M flow‑matching
    head). The paper claims state‑of‑the‑art results among autoregressive approaches
    and stronger image‑editing capabilities, and the authors say they will release
    code and models. Why it matters: diffusion models currently dominate image synthesis,
    but autoregressive approaches offer clearer conditional likelihoods and different
    tradeoffs for editing and compositionality. NextStep‑1’s use of continuous visual
    tokens at scale (rather than VQ/quantized tokens) is a notable architectural direction
    that could reopen high‑quality autoregressive image modeling at production scale.
    Impact: If reproducible, NextStep‑1 may shift some research and toolchains back
    toward autoregressive frameworks for tasks that benefit from token‑level control
    (precise editing, sequential multimodal generation) and will be of interest to
    researchers exploring alternatives to diffusion and hybrid generation schemes.'
  title: NextStep‑1 pushes autoregressive image generation with continuous tokens
    at scale
  url: https://arxiv.org/abs/2508.10711
- source: TechCrunch
  summary: 'Google rolled out Guided Learning inside Gemini (announced Aug 6, 2025):
    a study-focused mode that breaks problems into step-by-step lessons, uses images/diagrams/videos,
    and generates interactive quizzes, flashcards and study guides. Google is positioning
    Gemini as a learning assistant (not just an answer engine) and is pairing the
    feature with broader education support — including a free 12‑month AI Pro plan
    for eligible students and tighter integrations with NotebookLM and Gemini’s multimodal
    models. Why it matters: Guided Learning aims to make AI tools useful for actual
    learning and skill-building (useful for students, educators, and self-learners),
    and signals continued investment by major platforms to ship AI features tailored
    to education and skill development.'
  title: Google’s Gemini adds Guided Learning — an AI tutor that teaches you step-by-step
  url: https://techcrunch.com/2025/08/06/google-takes-on-chatgpts-study-mode-with-new-guided-learning-tool-in-gemini/
- source: Perplexity (Changelog)
  summary: 'Perplexity’s Aug 15 changelog announces a built-in video generation feature:
    Pro and Max subscribers can create 8‑second videos with audio (web, iOS, Android),
    powered by Veo 3 / Veo 3 Fast models and integrated into Perplexity’s workflows
    (Labs, Comet Assistant). The same update also expanded Perplexity Finance for
    Indian markets. Why it matters: adding native video (with audio) to an AI search/assistant
    platform turns research and ideation into shareable multimedia outputs — useful
    for creators, marketers, product teams and learners who want quick visualizations
    — and continues the shift toward multimodal, all‑in‑one AI productivity tools.'
  title: 'Perplexity ships built‑in AI video generation (Aug 15): short videos with
    audio for Pro/Max users'
  url: https://www.perplexity.ai/changelog/what-we-shipped-august-15th
summary: 'Google''s dropping $9 billion to beef up its AI and cloud setup in Oklahoma,
  which ties into their $1 billion education initiative. This isn''t just about shiny
  new data centers; it''s part of a bigger trend of keeping AI development close to
  home in the U.S.


  Meanwhile, Sam Altman''s throwing his weight behind Merge Labs to give Neuralink
  some competition in the brain-computer interface game. If Merge Labs can pull it
  off, we''re talking about a serious push for more advanced AI-neural hardware integration.


  On the research front, Microsoft and UW have introduced GFPO, a method to help large
  language models be less verbose and more efficient, which could make these models
  cheaper and faster to deploy. And in the image generation world, NextStep-'
---

<!-- Generated with AI web search 2025-08-17 13:33 UTC -->
