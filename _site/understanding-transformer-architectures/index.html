<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>Understanding Transformer Architectures - Giuseppe Concialdi</title>
  <meta name="description" content="The Transformer architecture revolutionized NLP by introducing the attention mechanism. This deep dive explores how transformers work, from self-attention to...">
  
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="http://localhost:4000/understanding-transformer-architectures/">
  
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Giuseppe Concialdi" />
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Understanding Transformer Architectures | Giuseppe Concialdi</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Understanding Transformer Architectures" />
<meta name="author" content="Giuseppe Concialdi" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The Transformer architecture revolutionized NLP by introducing the attention mechanism. This deep dive explores how transformers work, from self-attention to positional encoding, and why they’ve become the backbone of modern LLMs." />
<meta property="og:description" content="The Transformer architecture revolutionized NLP by introducing the attention mechanism. This deep dive explores how transformers work, from self-attention to positional encoding, and why they’ve become the backbone of modern LLMs." />
<link rel="canonical" href="http://localhost:4000/understanding-transformer-architectures/" />
<meta property="og:url" content="http://localhost:4000/understanding-transformer-architectures/" />
<meta property="og:site_name" content="Giuseppe Concialdi" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-01-23T09:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Understanding Transformer Architectures" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Giuseppe Concialdi"},"dateModified":"2025-01-23T09:00:00-05:00","datePublished":"2025-01-23T09:00:00-05:00","description":"The Transformer architecture revolutionized NLP by introducing the attention mechanism. This deep dive explores how transformers work, from self-attention to positional encoding, and why they’ve become the backbone of modern LLMs.","headline":"Understanding Transformer Architectures","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/understanding-transformer-architectures/"},"url":"http://localhost:4000/understanding-transformer-architectures/"}</script>
<!-- End Jekyll SEO tag -->

</head>

<body>
  <header class="site-header">
    <div class="wrapper">
      <a class="site-title" href="/">Giuseppe Concialdi</a>
      
      <nav class="site-nav">
        <a href="/" >Home</a>
        <a href="/writing/" >Writing</a>
        <a href="/about/" >About</a>
      </nav>
    </div>
  </header>

  <main class="page-content">
    <div class="wrapper">
      <article class="post">
  <header class="post-header">
    <h1 class="post-title">Understanding Transformer Architectures</h1>
    <div class="post-meta">
      <time datetime="2025-01-23T09:00:00-05:00">January 23, 2025</time>
       • <span>Giuseppe Concialdi</span>
    </div>
  </header>

  <div class="post-content">
    <p>The Transformer architecture, introduced in the landmark paper <a href="https://arxiv.org/abs/1706.03762">“Attention Is All You Need”</a>, fundamentally changed how we approach sequence modeling in natural language processing. Unlike RNNs or LSTMs that process tokens sequentially, transformers can process entire sequences in parallel through their attention mechanism.</p>

<h2 id="the-attention-revolution">The Attention Revolution</h2>

<p>Before transformers, recurrent models dominated sequence tasks but suffered from several limitations:</p>

<ul>
  <li><strong>Sequential Dependencies</strong>: RNNs process tokens one by one, making parallelization impossible</li>
  <li><strong>Long-Range Dependencies</strong>: Information from early tokens often gets lost in long sequences</li>
  <li><strong>Computational Inefficiency</strong>: Training becomes slow for long sequences due to sequential nature</li>
</ul>

<p>The transformer’s self-attention mechanism solves these problems by allowing each token to directly attend to all other tokens in the sequence, regardless of distance.</p>

<h2 id="core-architecture-components">Core Architecture Components</h2>

<h3 id="self-attention-mechanism">Self-Attention Mechanism</h3>

<p>Self-attention computes a weighted representation of all tokens in a sequence for each position. The mechanism uses three learned matrices: Query (Q), Key (K), and Value (V).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">"""
    Compute scaled dot-product attention.
    
    Args:
        Q: Query matrix [batch_size, seq_len, d_k]
        K: Key matrix [batch_size, seq_len, d_k]
        V: Value matrix [batch_size, seq_len, d_v]
        mask: Optional mask to prevent attention to certain positions
    """</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Compute attention scores
</span>    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
    
    <span class="c1"># Apply mask if provided
</span>    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
    
    <span class="c1"># Apply softmax to get attention weights
</span>    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Apply attention weights to values
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>
</code></pre></div></div>

<h3 id="multi-head-attention">Multi-Head Attention</h3>

<p>Instead of using a single attention function, transformers employ multiple “attention heads” that learn different types of relationships:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">query</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Linear transformations and split into heads
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_q</span><span class="p">(</span><span class="n">query</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_k</span><span class="p">(</span><span class="n">key</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">value</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_k</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Apply attention
</span>        <span class="n">attention_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        
        <span class="c1"># Concatenate heads and apply output projection
</span>        <span class="n">attention_output</span> <span class="o">=</span> <span class="n">attention_output</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">W_o</span><span class="p">(</span><span class="n">attention_output</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="positional-encoding">Positional Encoding</h3>

<p>Since transformers have no inherent notion of sequence order, positional encodings are added to input embeddings to provide positional information:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span> <span class="o">*</span> 
                           <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s">'pe'</span><span class="p">,</span> <span class="n">pe</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>
</code></pre></div></div>

<h2 id="from-transformers-to-modern-llms">From Transformers to Modern LLMs</h2>

<p>The original transformer was designed for machine translation, but the architecture has evolved significantly:</p>

<p><strong>GPT Series</strong>: Decoder-only transformers that excel at text generation</p>
<ul>
  <li>Use causal (masked) self-attention to prevent looking ahead</li>
  <li>Scale to billions of parameters with techniques like gradient checkpointing</li>
</ul>

<p><strong>BERT</strong>: Encoder-only transformers optimized for understanding</p>
<ul>
  <li>Bidirectional attention allows looking at full context</li>
  <li>Pre-trained with masked language modeling</li>
</ul>

<p><strong>T5 and Modern LLMs</strong>: Return to encoder-decoder architectures</p>
<ul>
  <li>Combine strengths of both encoder and decoder approaches</li>
  <li>Enable both understanding and generation tasks</li>
</ul>

<h2 id="key-innovations-in-modern-transformers">Key Innovations in Modern Transformers</h2>

<h3 id="attention-optimizations">Attention Optimizations</h3>

<p><strong>Flash Attention</strong>: Reduces memory usage and increases speed by fusing attention operations and using tiling techniques.</p>

<p><strong>Sparse Attention</strong>: Models like Longformer and BigBird use sparse attention patterns to handle longer sequences efficiently.</p>

<p><strong>Rotary Position Embedding (RoPE)</strong>: Used in models like LLaMA, provides better position understanding for longer sequences.</p>

<h3 id="scaling-techniques">Scaling Techniques</h3>

<p><strong>Mixture of Experts (MoE)</strong>: Only activate a subset of parameters for each input, allowing massive model scaling while maintaining efficiency.</p>

<p><strong>Gradient Checkpointing</strong>: Trade computation for memory by recomputing activations during backpropagation.</p>

<p><strong>Mixed Precision Training</strong>: Use lower precision arithmetic to speed up training while maintaining model quality.</p>

<h2 id="practical-considerations">Practical Considerations</h2>

<p>When working with transformer-based models:</p>

<ol>
  <li><strong>Memory Management</strong>: Attention has quadratic memory complexity in sequence length</li>
  <li><strong>Batch Size</strong>: Larger batches generally improve training stability and efficiency</li>
  <li><strong>Learning Rate Scheduling</strong>: Warmup followed by decay is crucial for stable training</li>
  <li><strong>Regularization</strong>: Dropout, layer normalization, and weight decay prevent overfitting</li>
</ol>

<h2 id="the-future-of-attention">The Future of Attention</h2>

<p>Current research directions include:</p>

<ul>
  <li><strong>Linear Attention</strong>: Reducing the quadratic complexity of standard attention</li>
  <li><strong>Retrieval-Augmented Models</strong>: Combining parametric knowledge with external retrieval</li>
  <li><strong>Multimodal Transformers</strong>: Extending attention to vision, audio, and other modalities</li>
  <li><strong>Efficient Architectures</strong>: Exploring alternatives like Mamba and other state-space models</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>The transformer architecture’s impact on AI cannot be overstated. From enabling the current generation of large language models to revolutionizing computer vision and other domains, attention-based models have become the foundation of modern AI systems.</p>

<p>Understanding transformers deeply—from the mathematical foundations to practical implementation details—is essential for anyone working with modern AI systems. As we continue to scale these models and explore new architectures, the core principles of attention and parallel processing remain central to progress in artificial intelligence.</p>

<hr />

<p><strong>Further Reading:</strong></p>
<ul>
  <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> - The original paper</li>
  <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> - Jay Alammar’s excellent visual guide</li>
  <li><a href="https://peterbloem.nl/blog/transformers">Transformers from Scratch</a> - Detailed implementation guide</li>
</ul>

  </div>

  <footer class="post-footer">
    <div class="post-navigation">
      
      
      
    </div>
    
    <div class="back-to-writing">
      <a href="/writing/">← Back to Writing</a>
    </div>
  </footer>
</article> 
    </div>
  </main>

  <footer class="site-footer">
    <div class="wrapper">
      <p>&copy; 2025 Giuseppe Concialdi. Machine Learning Engineer focused on LLMs and Generative AI.</p>
    </div>
  </footer>
</body>

</html>
